
@inproceedings{sulatycke_caching-efficient_1998,
	title = {Caching-efficient multithreaded fast multiplication of sparse matrices},
	url = {https://ieeexplore.ieee.org/document/669899},
	doi = {10.1109/IPPS.1998.669899},
	abstract = {Several fast sequential algorithms have been proposed in the past to multiply sparse matrices. These algorithms do not explicitly address the impact of caching on performance. We show that a rather simple sequential cache-efficient algorithm provides significantly better performance than existing algorithms for sparse matrix multiplication. We then describe a multithreaded implementation of this simple algorithm and show that its performance scales well with the number of threads and CPUs. For 10\% sparse, 500/spl times/500 matrices, the multithreaded version running on 4-CPU systems provides more than a 41.1-fold speed increase over the well-known BLAS routine and a 14.6 fold and 44.6-fold speed increase over two other recent techniques for fast sparse matrix multiplication, both of which are relatively difficult to parallelize efficiently.},
	urldate = {2025-04-18},
	booktitle = {Proceedings of the {First} {Merged} {International} {Parallel} {Processing} {Symposium} and {Symposium} on {Parallel} and {Distributed} {Processing}},
	author = {Sulatycke, P.D. and Ghose, K.},
	month = mar,
	year = {1998},
	note = {ISSN: 1063-7133},
	keywords = {Algorithm design and analysis, Computational fluid dynamics, Computer science, Data structures, Delay, Linear systems, Matrix converters, Sparse matrices, Symmetric matrices, Yarn},
	pages = {117--123},
	file = {Snapshot:C\:\\Users\\leona\\Zotero\\storage\\F3XUUTEV\\669899.html:text/html},
}

@techreport{yajnaseni_survey_2015,
	title = {A {Survey} on {Serial} and {Parallel} {Optimization} {Techniques} {Applicable} for {Matrix} {Multiplication} {Algorithm}},
	url = {https://www.researchgate.net/publication/285131617_A_Survey_on_Serial_and_Parallel_Optimization_Techniques_Applicable_for_Matrix_Multiplication_Algorithm},
	urldate = {2025-04-18},
	institution = {School of Studies inComputer Science \&IT, Pt. RavishankarShukla University,Raipur, Chhattisgarh,492010, India.},
	author = {Yajnaseni, Dash and Sanjay, Kumar and V.K., Patle},
	month = jan,
	year = {2015},
	file = {(PDF) A Survey on Serial and Parallel Optimization Techniques Applicable for Matrix Multiplication Algorithm:C\:\\Users\\leona\\Zotero\\storage\\GKMAPHY2\\285131617_A_Survey_on_Serial_and_Parallel_Optimization_Techniques_Applicable_for_Matrix_Multipl.html:text/html},
}

@article{tang_multithreaded_2024,
	title = {Multithreaded {Reproducible} {Banded} {Matrix}-{Vector} {Multiplication}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7390},
	url = {https://www.mdpi.com/2227-7390/12/3/422},
	doi = {10.3390/math12030422},
	abstract = {Reproducibility refers to getting bitwise identical floating point results from multiple runs of the same program, is an important basis for debugging or correctness checking in many codes. However the round-off error and non-associativity of floating point makes attaining reproducibility a challenge in large-scale, long-term parallel computing or solving ill conditioned problems. The dgbmv performs general banded matrix-vector multiplication for double precision, is the most basic Level-2 operation in BLAS. First, we designed a reproducible algorithm for banded matrix-vector multiplication repro\_dgbmv based on the technique of error-free transformation. Then the error of the algorithm is analyzed. Second, the algorithm is parallelized into repro\_dgbmv\_thread on ARM and x86 platforms. The numerical test results verify that repro\_dgbmv\_thread is reproducible and has higher accuracy than ordinary dgbmv. In numerical experiments on ARM platform, as the number of threads increases from 1 to 8, the run time of this algorithm is reduced by 5.2–7 times, while the run time of multithreaded dgbmv is only reduced by 2.2–3.8 times. In numerical experiments on x86 platform, as the number of threads increases from 1 to 15, the run time of this algorithm is reduced by 7.7–10.6 times, while the run time of multithreaded dgbmv is only reduced by 4.2–6.8 times.},
	language = {en},
	number = {3},
	urldate = {2025-04-18},
	journal = {Mathematics},
	author = {Tang, Tao and Qi, Haijun and Lu, Qingfeng and Jiang, Hao},
	month = jan,
	year = {2024},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {banded matrix, matrix-vector multiplication, multithreading, reproducibility},
	pages = {422},
	file = {Full Text PDF:C\:\\Users\\leona\\Zotero\\storage\\XBIMZCN4\\Tang et al. - 2024 - Multithreaded Reproducible Banded Matrix-Vector Multiplication.pdf:application/pdf},
}

@incollection{wyrzykowski_mathematical_2016,
	address = {Cham},
	title = {Mathematical {Approach} to the {Performance} {Evaluation} of {Matrix} {Multiply} {Algorithm}},
	volume = {9574},
	isbn = {978-3-319-32151-6 978-3-319-32152-3},
	url = {http://link.springer.com/10.1007/978-3-319-32152-3_3},
	language = {en},
	urldate = {2025-04-18},
	booktitle = {Parallel {Processing} and {Applied} {Mathematics}},
	publisher = {Springer International Publishing},
	author = {D’Amore, Luisa and Mele, Valeria and Laccetti, Giuliano and Murli, Almerico},
	editor = {Wyrzykowski, Roman and Deelman, Ewa and Dongarra, Jack and Karczewski, Konrad and Kitowski, Jacek and Wiatr, Kazimierz},
	year = {2016},
	doi = {10.1007/978-3-319-32152-3_3},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {25--34},
}

@article{he_-memory_2023,
	title = {In-memory computing based on phase change memory for high energy efficiency},
	volume = {66},
	issn = {1674-733X, 1869-1919},
	url = {https://link.springer.com/10.1007/s11432-023-3789-7},
	doi = {10.1007/s11432-023-3789-7},
	language = {en},
	number = {10},
	urldate = {2025-04-18},
	journal = {Sci. China Inf. Sci.},
	author = {He, Luchang and Li, Xi and Xie, Chenchen and Song, Zhitang},
	month = nov,
	year = {2023},
	pages = {200402},
}

@misc{abboud_time_2023,
	title = {The {Time} {Complexity} of {Fully} {Sparse} {Matrix} {Multiplication}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2309.06317},
	doi = {10.48550/ARXIV.2309.06317},
	abstract = {What is the time complexity of matrix multiplication of sparse integer matrices with \$m\_\{in\}\$ nonzeros in the input and \$m\_\{out\}\$ nonzeros in the output? This paper provides improved upper bounds for this question for almost any choice of \$m\_\{in\}\$ vs. \$m\_\{out\}\$, and provides evidence that these new bounds might be optimal up to further progress on fast matrix multiplication. Our main contribution is a new algorithm that reduces sparse matrix multiplication to dense (but smaller) rectangular matrix multiplication. Our running time thus depends on the optimal exponent \$ω(a,b,c)\$ of multiplying dense \$n{\textasciicircum}a{\textbackslash}times n{\textasciicircum}b\$ by \$n{\textasciicircum}b{\textbackslash}times n{\textasciicircum}c\$ matrices. We discover that when \$m\_\{out\}=Θ(m\_\{in\}{\textasciicircum}r)\$ the time complexity of sparse matrix multiplication is \$O(m\_\{in\}{\textasciicircum}\{σ+ε\})\$, for all \$ε\&gt; 0\$, where \$σ\$ is the solution to the equation \$ω(σ-1,2-σ,1+r-σ)=σ\$. No matter what \$ω({\textbackslash}cdot,{\textbackslash}cdot,{\textbackslash}cdot)\$ turns out to be, and for all \$r{\textbackslash}in(0,2)\$, the new bound beats the state of the art, and we provide evidence that it is optimal based on the complexity of the all-edge triangle problem. In particular, in terms of the input plus output size \$m = m\_\{in\} + m\_\{out\}\$ our algorithm runs in time \$O(m{\textasciicircum}\{1.3459\})\$. Even for Boolean matrices, this improves over the previous \$m{\textasciicircum}\{{\textbackslash}frac\{2ω\}\{ω+1\}+ε\}=O(m{\textasciicircum}\{1.4071\})\$ bound [Amossen, Pagh; 2009], which was a natural barrier since it coincides with the longstanding bound of all-edge triangle in sparse graphs [Alon, Yuster, Zwick; 1994]. We find it interesting that matrix multiplication can be solved faster than triangle detection in this natural setting. In fact, we establish an equivalence to a special case of the all-edge triangle problem.},
	urldate = {2025-04-19},
	publisher = {arXiv},
	author = {Abboud, Amir and Bringmann, Karl and Fischer, Nick and Künnemann, Marvin},
	year = {2023},
	note = {Version Number: 1},
	keywords = {Data Structures and Algorithms (cs.DS), FOS: Computer and information sciences},
}

@article{bhutani_exploring_2024,
	title = {Exploring the {Impact} of {Multithreading} on {System} {Resource} {Utilization} and {Efficiency}},
	volume = {11},
	issn = {23500557},
	url = {https://ijirem.org/view_abstract.php?title=Exploring-the-Impact-of-Multithreading-on-System-Resource-Utilization-and-Efficiency&year=2024&vol=11&primary=QVJULTE4MjM=},
	doi = {10.55524/ijirem.2024.11.5.9},
	abstract = {The goal of this research is to find out the effects of multithreading on the consumption of system resources and efficiency by examining CPU utilization, memory use, Input/Output operation, and power consumption in the multithreaded systems. In order to measure static, adaptive, and dynamic multithreading performance under different workloads, the study compares the three models both through theory and by applying it to various experiments. That is why the results show that, when multithreading is implemented, the general CPU load and I/O performance improve, especially for computational and data-consuming operations. However, some problems include memory contention, context-switching overhead and, higher energy consumption are noted especially when threading is over-provisioned for. Real-time threading control strategies were the most effective as they periodically reconfigured the number of threads in a way that optimizes performance while optimizing resources. In addition, while the asynchronous I/O models provided the best performance improvements, energy use went up when multithreading was incorporated, thus the need for implementing trade-offs between performance and power usage. It offers significant findings related to the tuning of multithreading strategies so as to maximize system performance but with an acceptable level of resource utilization, should be of significance for practitioners, who are working with multicore processors and dealing with high-performing systems.},
	number = {5},
	urldate = {2025-04-19},
	journal = {IJIREM},
	author = {Bhutani, Preet and Shinde, Amol Ashokrao},
	month = oct,
	year = {2024},
	pages = {66--72},
	file = {Full Text PDF:C\:\\Users\\leona\\Zotero\\storage\\LB2IRGWF\\Bhutani e Shinde - 2024 - Exploring the Impact of Multithreading on System Resource Utilization and Efficiency.pdf:application/pdf},
}

@incollection{cooper_how_2011,
	address = {Berlin, Heidelberg},
	title = {How {Many} {Threads} to {Spawn} during {Program} {Multithreading}?},
	volume = {6548},
	isbn = {978-3-642-19594-5 978-3-642-19595-2},
	url = {http://link.springer.com/10.1007/978-3-642-19595-2_12},
	urldate = {2025-04-19},
	booktitle = {Languages and {Compilers} for {Parallel} {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Nicolau, Alexandru and Kejariwal, Arun},
	editor = {Cooper, Keith and Mellor-Crummey, John and Sarkar, Vivek},
	year = {2011},
	doi = {10.1007/978-3-642-19595-2_12},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {166--183},
}


@article{gallo_overview_2020,
	title = {An overview of phase-change memory device physics},
	volume = {53},
	issn = {0022-3727},
	url = {https://iopscience.iop.org/article/10.1088/1361-6463/ab7794/meta},
	doi = {10.1088/1361-6463/ab7794},
	abstract = {An overview of phase-change memory device physics, Le Gallo, Manuel, Sebastian, Abu},
	language = {en},
	number = {21},
	urldate = {2025-09-12},
	journal = {J. Phys. D: Appl. Phys.},
	author = {Gallo, Manuel Le and Sebastian, Abu},
	month = mar,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {213002},
}

@article{antolini_readout_2024,
	title = {A {Readout} {Scheme} for {PCM}-{Based} {Analog} {In}-{Memory} {Computing} {With} {Drift} {Compensation} {Through} {Reference} {Conductance} {Tracking}},
	volume = {4},
	issn = {2644-1349},
	url = {https://ieeexplore.ieee.org/document/10609348},
	doi = {10.1109/OJSSCS.2024.3432468},
	abstract = {This article presents a readout scheme for analog in-memory computing (AIMC) based on an embedded phase-change memory (ePCM). Conductance time drift is overcome with a hardware compensation technique based on a reference cell conductance tracking (RCCT). Accuracy drop due to circuits mismatch and variability involved in the computational chain are minimized with an optimized iterative program-and-verify algorithm applied to the phase-change memory (PCM) devices. The proposed AIMC scheme is designed and manufactured in a 90-nm STMicroelectronics CMOS technology, with the aim of adding a signed multiply-and-accumulate (MAC) computation feature to a Ge-Rich GeSbTe (GST) embedded PCM array. Experimental characterizations are performed under different operating conditions and show that the mean MAC decrease in time is approximately null at room temperature and reduced by a factor of 3 after 64-h bake at 85 {\textasciicircum}{\textbackslash}circ C. Based on several MAC operations, the estimated 512{\textbackslash}times 512 matrix-vector-multiplication (MVM) accuracy is 97.4\%, whose decrease in time is less than 3\% in the worst case.},
	urldate = {2025-09-12},
	journal = {IEEE Open Journal of the Solid-State Circuits Society},
	author = {Antolini, Alessio and Lico, Andrea and Zavalloni, Francesco and Scarselli, Eleonora Franchi and Gnudi, Antonio and Torres, Mattia Luigi and Canegallo, Roberto and Pasotti, Marco},
	year = {2024},
	keywords = {Accuracy, Analog in-memory computing (AIMC), artificial intelligence, Artificial intelligence, drift compensation, In-memory computing, matrix-vector multiplication (MVM), Nonvolatile memory, Phase change materials, Phase change random access memory, phase-change memory (PCM), Programming, Solid state circuits, Temperature measurement},
	pages = {69--82},
	file = {Full Text PDF:C\:\\Users\\leona\\Zotero\\storage\\UKBWPZ77\\Antolini et al. - 2024 - A Readout Scheme for PCM-Based Analog In-Memory Computing With Drift Compensation Through Reference.pdf:application/pdf},
}


@article{wuttig_phase-change_2007,
	title = {Phase-change materials for rewriteable data storage},
	volume = {6},
	copyright = {2007 Springer Nature Limited},
	issn = {1476-4660},
	url = {https://www.nature.com/articles/nmat2009},
	doi = {10.1038/nmat2009},
	abstract = {Phase-change materials are some of the most promising materials for data-storage applications. They are already used in rewriteable optical data storage and offer great potential as an emerging non-volatile electronic memory. This review looks at the unique property combination that characterizes phase-change materials. The crystalline state often shows an octahedral-like atomic arrangement, frequently accompanied by pronounced lattice distortions and huge vacancy concentrations. This can be attributed to the chemical bonding in phase-change alloys, which is promoted by p-orbitals. From this insight, phase-change alloys with desired properties can be designed. This is demonstrated for the optical properties of phase-change alloys, in particular the contrast between the amorphous and crystalline states. The origin of the fast crystallization kinetics is also discussed.},
	language = {en},
	number = {11},
	urldate = {2025-09-12},
	journal = {Nature Mater},
	author = {Wuttig, Matthias and Yamada, Noboru},
	month = nov,
	year = {2007},
	note = {Publisher: Nature Publishing Group},
	keywords = {Biomaterials, Condensed Matter Physics, general, Materials Science, Nanotechnology, Optical and Electronic Materials},
	pages = {824--832},
	file = {Full Text PDF:C\:\\Users\\leona\\Zotero\\storage\\3LY2FZGH\\Wuttig e Yamada - 2007 - Phase-change materials for rewriteable data storage.pdf:application/pdf},
}

@article{mannocci_-memory_2023,
	title = {In-memory computing with emerging memory devices: {Status} and outlook},
	volume = {1},
	issn = {2770-9019},
	shorttitle = {In-memory computing with emerging memory devices},
	url = {https://pubs.aip.org/aml/article/1/1/010902/2878744/In-memory-computing-with-emerging-memory-devices},
	doi = {10.1063/5.0136403},
	abstract = {In-memory computing (IMC) has emerged as a new computing paradigm able to alleviate or suppress the memory bottleneck, which is the major concern for energy efficiency and latency in modern digital computing. While the IMC concept is simple and promising, the details of its implementation cover a broad range of problems and solutions, including various memory technologies, circuit topologies, and programming/processing algorithms. This Perspective aims at providing an orientation map across the wide topic of IMC. First, the memory technologies will be presented, including both conventional complementary metal-oxide-semiconductor-based and emerging resistive/memristive devices. Then, circuit architectures will be considered, describing their aim and application. Circuits include both popular crosspoint arrays and other more advanced structures, such as closed-loop memory arrays and ternary content-addressable memory. The same circuit might serve completely different applications, e.g., a crosspoint array can be used for accelerating matrix-vector multiplication for forward propagation in a neural network and outer product for backpropagation training. The different algorithms and memory properties to enable such diversification of circuit functions will be discussed. Finally, the main challenges and opportunities for IMC will be presented.},
	language = {en},
	number = {1},
	urldate = {2025-09-12},
	journal = {APL Machine Learning},
	author = {Mannocci, P. and Farronato, M. and Lepri, N. and Cattaneo, L. and Glukhov, A. and Sun, Z. and Ielmini, D.},
	month = mar,
	year = {2023},
	pages = {010902},
	file = {Full Text PDF:C\:\\Users\\leona\\Zotero\\storage\\6VSSM7YZ\\Mannocci et al. - 2023 - In-memory computing with emerging memory devices Status and outlook.pdf:application/pdf},
}


@misc{gholami_ai_2024,
	title = {{AI} and {Memory} {Wall}},
	url = {http://arxiv.org/abs/2403.14123},
	doi = {10.48550/arXiv.2403.14123},
	abstract = {The availability of unprecedented unsupervised training data, along with neural scaling laws, has resulted in an unprecedented surge in model size and compute requirements for serving/training LLMs. However, the main performance bottleneck is increasingly shifting to memory bandwidth. Over the past 20 years, peak server hardware FLOPS has been scaling at 3.0x/2yrs, outpacing the growth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and 1.4 times every 2 years, respectively. This disparity has made memory, rather than compute, the primary bottleneck in AI applications, particularly in serving. Here, we analyze encoder and decoder Transformer models and show how memory bandwidth can become the dominant bottleneck for decoder models. We argue for a redesign in model architecture, training, and deployment strategies to overcome this memory limitation.},
	urldate = {2025-09-12},
	publisher = {arXiv},
	author = {Gholami, Amir and Yao, Zhewei and Kim, Sehoon and Hooper, Coleman and Mahoney, Michael W. and Keutzer, Kurt},
	month = mar,
	year = {2024},
	note = {arXiv:2403.14123 [cs]
version: 1},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture, Computer Science - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\leona\\Zotero\\storage\\RU4YAYYD\\Gholami et al. - 2024 - AI and Memory Wall.pdf:application/pdf;Snapshot:C\:\\Users\\leona\\Zotero\\storage\\LGEUWDGC\\2403.html:text/html},
}

@misc{khan_landscape_2024,
	title = {The {Landscape} of {Compute}-near-memory and {Compute}-in-memory: {A} {Research} and {Commercial} {Overview}},
	shorttitle = {The {Landscape} of {Compute}-near-memory and {Compute}-in-memory},
	url = {http://arxiv.org/abs/2401.14428},
	doi = {10.48550/arXiv.2401.14428},
	abstract = {In today's data-centric world, where data fuels numerous application domains, with machine learning at the forefront, handling the enormous volume of data efficiently in terms of time and energy presents a formidable challenge. Conventional computing systems and accelerators are continually being pushed to their limits to stay competitive. In this context, computing near-memory (CNM) and computing-in-memory (CIM) have emerged as potentially game-changing paradigms. This survey introduces the basics of CNM and CIM architectures, including their underlying technologies and working principles. We focus particularly on CIM and CNM architectures that have either been prototyped or commercialized. While surveying the evolving CIM and CNM landscape in academia and industry, we discuss the potential benefits in terms of performance, energy, and cost, along with the challenges associated with these cutting-edge computing paradigms.},
	urldate = {2025-09-12},
	publisher = {arXiv},
	author = {Khan, Asif Ali and Lima, João Paulo C. De and Farzaneh, Hamid and Castrillon, Jeronimo},
	month = jan,
	year = {2024},
	note = {arXiv:2401.14428 [cs]
version: 1},
	keywords = {Computer Science - Hardware Architecture},
	file = {Full Text PDF:C\:\\Users\\leona\\Zotero\\storage\\NCB5SS5M\\Khan et al. - 2024 - The Landscape of Compute-near-memory and Compute-in-memory A Research and Commercial Overview.pdf:application/pdf;Snapshot:C\:\\Users\\leona\\Zotero\\storage\\98Y8Z3EU\\2401.html:text/html},
}

@article{sebastian_memory_2020,
	title = {Memory devices and applications for in-memory computing},
	volume = {15},
	copyright = {2020 Springer Nature Limited},
	issn = {1748-3395},
	url = {https://www.nature.com/articles/s41565-020-0655-z},
	doi = {10.1038/s41565-020-0655-z},
	abstract = {Traditional von Neumann computing systems involve separate processing and memory units. However, data movement is costly in terms of time and energy and this problem is aggravated by the recent explosive growth in highly data-centric applications related to artificial intelligence. This calls for a radical departure from the traditional systems and one such non-von Neumann computational approach is in-memory computing. Hereby certain computational tasks are performed in place in the memory itself by exploiting the physical attributes of the memory devices. Both charge-based and resistance-based memory devices are being explored for in-memory computing. In this Review, we provide a broad overview of the key computational primitives enabled by these memory devices as well as their applications spanning scientific computing, signal processing, optimization, machine learning, deep learning and stochastic computing.},
	language = {en},
	number = {7},
	urldate = {2025-09-12},
	journal = {Nat. Nanotechnol.},
	author = {Sebastian, Abu and Le Gallo, Manuel and Khaddam-Aljameh, Riduan and Eleftheriou, Evangelos},
	month = jul,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	keywords = {Electrical and electronic engineering, Electronic devices, Information storage},
	pages = {529--544},
	file = {Full Text PDF:C\:\\Users\\leona\\Zotero\\storage\\X4KX4ZVA\\Sebastian et al. - 2020 - Memory devices and applications for in-memory computing.pdf:application/pdf},
}


@inproceedings{bruschi_gvsoc_2021,
	title = {{GVSoC}: {A} {Highly} {Configurable}, {Fast} and {Accurate} {Full}-{Platform} {Simulator} for {RISC}-{V} based {IoT} {Processors}},
	shorttitle = {{GVSoC}},
	url = {http://arxiv.org/abs/2201.08166},
	doi = {10.1109/ICCD53106.2021.00071},
	abstract = {The last few years have seen the emergence of IoT processors: ultra-low power systems-on-chips (SoCs) combining lightweight and flexible micro-controller units (MCUs), often based on open-ISA RISC-V cores, with application-specific accelerators to maximize performance and energy efficiency. Overall, this heterogeneity level requires complex hardware and a full-fledged software stack to orchestrate the execution and exploit platform features. For this reason, enabling agile design space exploration becomes a crucial asset for this new class of low-power SoCs. In this scenario, high-level simulators play an essential role in breaking the speed and design effort bottlenecks of cycle-accurate simulators and FPGA prototypes, respectively, while preserving functional and timing accuracy. We present GVSoC, a highly configurable and timing-accurate event-driven simulator that combines the efficiency of C++ models with the flexibility of Python configuration scripts. GVSoC is fully open-sourced, with the intent to drive future research in the area of highly parallel and heterogeneous RISC-V based IoT processors, leveraging three foundational features: Python-based modular configuration of the hardware description, easy calibration of platform parameters for accurate performance estimation, and high-speed simulation. Experimental results show that GVSoC enables practical functional and performance analysis and design exploration at the full-platform level (processors, memory, peripherals and IOs) with a speed-up of 2500x with respect to cycle-accurate simulation with errors typically below 10\% for performance analysis.},
	urldate = {2025-09-12},
	booktitle = {2021 {IEEE} 39th {International} {Conference} on {Computer} {Design} ({ICCD})},
	author = {Bruschi, Nazareno and Haugou, Germain and Tagliavini, Giuseppe and Conti, Francesco and Benini, Luca and Rossi, Davide},
	month = oct,
	year = {2021},
	note = {arXiv:2201.08166 [eess]},
	keywords = {Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control},
	pages = {409--416},
	file = {Preprint PDF:C\:\\Users\\leona\\Zotero\\storage\\4T4DJ4EA\\Bruschi et al. - 2021 - GVSoC A Highly Configurable, Fast and Accurate Full-Platform Simulator for RISC-V based IoT Process.pdf:application/pdf;Snapshot:C\:\\Users\\leona\\Zotero\\storage\\NRJ3CFQ4\\2201.html:text/html},
}


@article{srinivasan_study_2013,
	title = {A {Study} on the {Challenges} and {Prospects} of {PCM} {Based} {Main} {Memory} {Architectures}},
	volume = {18},
	doi = {10.5829/idosi.mejsr.2013.18.6.12500},
	abstract = {For several decades, computer's memory has been volatile. High speed memories such as DRAM and SRAM have been used as main memory and cache memory respectively. Magnetic disks are used as the persistent secondary storage devices. The present DRAM based main memory has reached its energy lits. Due to the advent of nonvolatile memories like PCM, MRAM, RRAM, FeRAM and Flash, there is growing interest in incorporating them into the computers as the main memory alternates. Introducing the non-volatile storage devices in memory hierarchy is considered to reduce the energy consumption since non-volatile memories have got no power leakage in memory cells. A major hurdle in this direction is the poor the endurance limit of most non-volatile memory technologies in comparison to the RAM technologies. Among the non-volatile memory technologies, PCM is the most attractive and the most investigated option due to it advantages over other memory types. But there are few issues related to PCM that need to be addressed in order to integrate it as a potential memory for computers. There have been intense researches being carried out to devise new memory architectures based on PCM. There are also methods identified to overcome the issues of PCM. This paper analyses the various challenges in integrating PCM into the system and the various PCM based main memory architectures that have been proposed so far. INTRODUCTION memory capacity. A number of non-volatile memories are},
	journal = {Middle East Journal of Scientific Research},
	author = {Srinivasan, Rajarajan and Prabhu, M. and Arunkumar, Sabari and Karthikeyan, M},
	month = dec,
	year = {2013},
	pages = {788--795},
	file = {Full Text PDF:C\:\\Users\\leona\\Zotero\\storage\\43GV494A\\Srinivasan et al. - 2013 - A Study on the Challenges and Prospects of PCM Based Main Memory Architectures.pdf:application/pdf},
}
